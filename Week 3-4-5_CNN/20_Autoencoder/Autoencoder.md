# ğŸ§  **Autoencoder: Architettura, Funzionamento e Applicazioni**

## 1ï¸âƒ£ **Cosâ€™Ã¨ un Autoencoder?**

Un **autoencoder** Ã¨ una **rete neurale artificiale** utilizzata principalmente per:

* ğŸ” **Task non supervisionati**
* ğŸ“‰ **Riduzione della dimensionalitÃ ** (Dimensionality Reduction)
* ğŸ”„ **Ricostruzione dei dati di input**

**Caratteristica principale:**
Impara a **comprimere** i dati in una rappresentazione latente piÃ¹ compatta (encoding) e poi a **ricostruire** i dati originali (decoding), tutto **senza bisogno di etichette** (apprendimento non supervisionato).

---

## 2ï¸âƒ£ **Architettura di un Autoencoder**

### **Componenti principali**

* **Encoder** ğŸ§¬

  * Riceve i dati di input
  * Li trasforma in una rappresentazione a **dimensione ridotta** (**spazio latente**)
  * Composto da uno o piÃ¹ layer che riducono progressivamente la dimensionalitÃ 

* **Spazio Latente** ğŸ’ 

  * Rappresentazione **compressa** dei dati
  * Contiene solo le feature **piÃ¹ rilevanti** ed essenziali

* **Decoder** ğŸ”

  * Prende la rappresentazione dallo spazio latente
  * Prova a **ricostruire** il dato nello spazio originale
  * Anche qui, uno o piÃ¹ layer **espandono** progressivamente la dimensionalitÃ 

---

## 3ï¸âƒ£ **Funzionamento: Come â€œImparaâ€ un Autoencoder?**

* **Obiettivo del training** ğŸ¯:
  Minimizzare lâ€™**errore di ricostruzione** tra input originale e output ricostruito (solitamente tramite funzione di costo come **MSE â€“ Mean Squared Error**).

* **Step fondamentali**

  1. **Feature Extraction** ğŸ§²
     Lâ€™encoder estrae e trattiene le informazioni piÃ¹ importanti dei dati.
  2. **Compressione nello spazio latente** ğŸ’¾
     I dati vengono compressi, mantenendo solo ciÃ² che Ã¨ rilevante.
  3. **Ricostruzione** ğŸ› ï¸
     Il decoder prova a ricostruire i dati originali partendo dalla rappresentazione compressa. I dati ricostruiti non sono identici ma molto simili agli originali.

![alt](../../images/autoencoder.png)

* **Addestramento** ğŸ”„

  * Si passa lâ€™input nellâ€™encoder â†’ compressione â†’ spazio latente
  * Lo spazio latente passa nel decoder â†’ ricostruzione output
  * Calcolo errore â†’ **Backpropagation** aggiorna i pesi (sia dell'encoder che del decoder) per minimizzare la differenza tra input e output, ovvero minimizzare la funzione di costo
  * Questo ciclo viene ripetuto per molte epoche, fino ad ottenere ricostruzioni soddisfacenti, cioÃ¨ quando il valore della funzione di costo Ã¨ per noi accettabile

---

## 4ï¸âƒ£ **PerchÃ© funziona? Lâ€™Intuizione del â€œCollo di Bottigliaâ€** ğŸ¼

* Lâ€™autoencoder crea un **bottleneck** (collo di bottiglia), costringendo la rete a passare solo le **informazioni piÃ¹ rilevanti** nello spazio latente
* In questo modo, il modello:

  * Impara a **scartare il rumore**
  * Trattiene solo le **feature fondamentali**
* Lâ€™architettura favorisce quindi una **compressione efficiente** e una rappresentazione dei dati piÃ¹ robusta

---

## 5ï¸âƒ£ **Applicazioni Principali degli Autoencoder** ğŸš€

1. **Dimensionality Reduction** ğŸ“‰

   * Riducono la dimensionalitÃ  dei dati senza supervisione
   * Utile per **visualizzazione**, **compressione dati** e **pre-processing** di modelli successivi

2. **Anomaly Detection** âš ï¸

   * Allenando un autoencoder solo su dati â€œnormaliâ€ (senza anomalie), un errore di ricostruzione elevato indica probabili anomalie nei dati in input

3. **Denoising** ğŸ”Šâ¡ï¸ğŸ¤«

   * Rimozione del rumore dai dati (es. immagini â€œsporcheâ€ â†’ immagini pulite)
   * Addestrato a ricostruire dati puliti da input rumorosi

4. **Compressione e Generazione di Immagini** ğŸ—œï¸ğŸ–¼ï¸

   * Compressione efficiente di immagini
   * Varianti (es. Variational Autoencoder) sono capaci di **generare nuovi dati** simili a quelli visti in training

---

## 6ï¸âƒ£ **Composizione dell'Autoencoder

* Un Autoencoder Ã¨ formato da due reti distinte
  - Una per l'encoding: prende in input il dato e lo trasporta nello sapzio latente
  - Una per il decoding: parte dallo spazio latente e ricostruisce il dato nello spazio di input
* Queste due reti distinte non devono necessariamente lavorare insieme, l'importante Ã¨ che entrambe le reti siano addestrate sugli stessi dati.
  - Ad es. Su una macchina abbiamo l'ecoder, su un'altra il decoder

---

## 7ï¸âƒ£ **Varianti degli Autoencoder** ğŸ§¬

* **Sparse Autoencoder**
  Introducono un vincolo di **sparsitÃ ** nella loss function â†’ la rappresentazione latente contiene molti zeri, rendendola piÃ¹ compatta

* **Denoising Autoencoder**
  Specializzati nel rimuovere il rumore dai dati di input

* **Variational Autoencoder (VAE)**
  Introducono una modellazione **probabilistica** nello spazio latente â†’ usati per **generazione di nuovi dati** e **data augmentation**

* **Contractive Autoencoder**
  Aggiungono regolarizzazione per rendere lo spazio latente **robusto a piccole variazioni** nellâ€™input

---

## 8ï¸âƒ£ **Caratteristiche Chiave dello Spazio Latente** ğŸ’ 

* **Dimensione Ridotta** ğŸ“
  Molto piÃ¹ piccola rispetto ai dati originali grazie ai vari passaggi di encoding e pooling

* **Feature Importanti** ğŸŒŸ
  Si conservano solo le informazioni essenziali e i pattern principali

* **Compressione â€œLossyâ€** âš ï¸
  Alcune informazioni vengono perse (non tutta la ricostruzione Ã¨ perfetta), ma si mantengono le caratteristiche principali

* **Data Specific** ğŸ§¬
  Gli autoencoder funzionano meglio su dati **simili** a quelli usati per lâ€™addestramento

---

## 9ï¸âƒ£ **Processo di Addestramento** ğŸ”

1. Passaggio dellâ€™input nellâ€™encoder
2. Compressione nello spazio latente (bottleneck)
3. Decodifica per ricostruire lâ€™input
4. Calcolo dellâ€™errore di ricostruzione (**es. MSE**)
5. Backpropagation per aggiornare i pesi
6. Ripetizione del ciclo per molte epoche

---

## ğŸ”Ÿ **Limiti e Considerazioni** ğŸ›‘

* La **ricostruzione non sarÃ  mai perfetta**: la compressione comporta perdita di informazioni (lossy). L'output sarÃ  sicuramente diversa dall'input (anche solo per un bit)
* **Funzionano meglio** se dati di test sono simili ai dati di training (data-specific)
* Sono **non supervisionati**: non necessitano di etichette (labels) per lâ€™addestramento

---

## 11 **Riepilogo Visuale** ğŸ“

```
Input Data â†’ [Encoder] â†’ [Spazio Latente] â†’ [Decoder] â†’ Output Ricostruito
        |----------- Minimizzare lâ€™errore di ricostruzione -----------|
```

---

## ğŸ” **Riassunto finale**

Gli **autoencoder** sono **architetture fondamentali** nel machine learning e nel deep learning, soprattutto per **estrarre** le informazioni piÃ¹ importanti da grandi quantitÃ  di dati, **ricostruire** input, **scovare anomalie**, **ripulire dati rumorosi** e **generare** nuovi dati.
Grazie al **collo di bottiglia**, imparano rappresentazioni **efficienti**, seppur â€œlossyâ€, dei dati originali, risultando strumenti estremamente **versatili** per moltissimi task nel mondo dellâ€™AI. ğŸš€

---
