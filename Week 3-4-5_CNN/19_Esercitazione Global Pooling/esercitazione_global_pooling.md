# ğŸ§  Confronto tra due reti CNN: con e senza Global Pooling

## ğŸ“ Dataset utilizzato: `CatsVsDogs` (TensorFlow Datasets)

* âœ… Usato in modalitÃ  `as_supervised`: ogni elemento Ã¨ una **tupla (immagine, label)**.
* âœ… La label:

  * `1` = ğŸ¶ **Cane**
  * `0` = ğŸ± **Gatto**
* ğŸ”¢ Numero di esempi: **20.936**

---

## ğŸ§¹ Preprocessing immagini

* ğŸ§­ Cast a `float32`
* âš–ï¸ Normalizzazione pixel tra `0` e `1`
* ğŸ“ Resize intelligente con funzione TensorFlow
* ğŸ“¦ Divisione in `train_dataset` e `test_dataset`

---

## ğŸ§ª Differenze nella preparazione

### ğŸ”¬ Dataset di **test**:

* Solo specifica `batch size` (una sola epoca, semplice)

### ğŸ‹ï¸ Dataset di **train**:

* Shuffle con buffer pari al numero di esempi
* `repeat()` â• `shuffle()` ogni epoca
* `batch_size = 64`
* `steps_per_epoch = num_examples // batch_size` â¡ï¸ calcolato dinamicamente
* âš ï¸ **Nessun caricamento in RAM iniziale**: utilizzo intelligente di `prefetch`, abbiamo specificato come preprocessare i dataset di train e di validation e di come deve essere fatta la fase di training (dimensione del batch, steps per epoca e rimescolamento delle immagini) ma senza che nessuna immagine sia ancora caricata in memoria. Questo Ã¨ il vantaggio di usare **tensorflow-datasets** perchÃ¨ usa un dataset di `prefetch` in maniera intelligente dato che alcuni dataset potebbero essere enormi e non starci nella memoria.

---

## ğŸ§± Architettura della rete **senza** Global Pooling

### ğŸ”„ Rete `Sequential`:

* 3ï¸âƒ£ Blocchi convolutivi (Conv2D â• BatchNorm â• ReLU â• MaxPooling)
* ğŸ§¯ Flatten
* ğŸ”¢ 2 livelli `Dense` (il primo con 64 neuroni, il secondo con 1 neurone â• `sigmoid`)

### ğŸ§® Parametri:

* ğŸ”¢ Totali: **\~900.000**
* ğŸ§¨ \~820.000 solo nel livello `Dense`
* Motivo: `Flatten` produce **12.800** valori (da feature map 10x10x128)

### âš™ï¸ Addestramento:

* `25 epoch`
* `fit()` con steps predefiniti
* ğŸš€ Accuracy finale: **99%**
* âŒ Accuracy in fase di test: **84,9%**

---

## Exursus Global Pooling

## ğŸ§  Cos'Ã¨ il Global Pooling?

ğŸ”¹ Ãˆ un'operazione che **trasforma ogni feature map (H Ã— W)** in **un singolo numero**, **collassando lâ€™informazione spaziale** su tutta la mappa.

Due versioni comuni:

* **Global Average Pooling (GAP)** â†’ fa la media di tutti i valori di ciascuna feature map
* **Global Max Pooling (GMP)** â†’ prende il valore massimo di ciascuna feature map

---

## ğŸ“¦ Esempio concreto

Supponiamo di avere una feature map di **dimensioni 7Ã—7Ã—128**, cioÃ¨:

* 128 feature maps (canali)
* Ognuna di 7Ã—7 pixel

### Dopo `GlobalAveragePooling2D()`:

ğŸ‘‰ Ottieni un vettore **1Ã—1Ã—128**

---

## ğŸ“ Cosa succede all'informazione spaziale?

| Aspetto                                                  | Cosa succede con Global Pooling   |
| -------------------------------------------------------- | --------------------------------- |
| **Informazione spaziale locale (posizione, forma)**      | âŒ Persa                           |
| **Presenza o attivazione di pattern** (es. linee, bordi) | âœ… Mantenuta                       |
| **Efficienza computazionale**                            | âœ… Molto alta                      |
| **Numero di parametri**                                  | âœ… Nessuno (Ã¨ un'operazione fissa) |

---

## ğŸ¯ Quando usarlo?

âœ… Quando:

* Vuoi **ridurre drasticamente la dimensionalitÃ ** prima di un layer denso
* Lâ€™importante Ã¨ **il â€œcosa câ€™Ã¨â€**, non **â€œdoveâ€ Ã¨**
* Usi architetture **senza Flatten + Dense**, piÃ¹ leggere (es. MobileNet)

âŒ Evita se:

* Devi **preservare posizione** o **relazioni spaziali** (es. segmentazione, detection)

---

## ğŸ†š Global Pooling vs Flatten

| Caratteristica           | Global Pooling | Flatten + Dense            |
| ------------------------ | -------------- | -------------------------- |
| Mantiene spatial layout? | âŒ No           | âœ… SÃ¬ (tramite connessioni) |
| Parametri?               | 0              | Molti                      |
| Rischio overfitting      | Basso          | PiÃ¹ alto                   |
| Performance              | PiÃ¹ efficiente | PiÃ¹ potente (ma costoso)   |

---

## ğŸ§© Conclusione

Il **Global Pooling**:

* **Comprima** ogni feature map in un singolo valore
* **Riduce drasticamente la dimensione**
* **Perde la posizione esatta** delle attivazioni
* Ãˆ molto usato in **modelli leggeri o di classificazione pura**

Se hai bisogno di **preservare la geometria o la forma precisa di un oggetto**, meglio usare `Flatten()` e `Dense`.

---


## ğŸŒ€ Rete **con Global Pooling**

### ğŸ§  Sostituzioni:

* ğŸ” Rimosso flatten e i due layer `Dense`
* âœ¨ Inserito `GlobalMaxPooling2D`
* â• Output diretto a `Dense(1, activation='sigmoid')`

### ğŸ“‰ Parametri:

* ğŸ”¢ Totali: **\~94.000**
* ğŸ’¡ Nessun parametro nel `GlobalMaxPooling` (Ã¨ un'operazione aritmetica, non apprende)

### ğŸ” Cosa fa il `Global Max Pooling`?

* Prende per ogni **feature map (10x10)** il **massimo valore**
* Output finale: **128 valori** (uno per ciascuna feature map)
* Evita il flatten e la moltiplicazione dei parametri â—

---

## ğŸ“Š Confronto delle performance

| ğŸ”§ Architettura              | Parametri totali | Test Accuracy | Loss in test |
| ---------------------------- | ---------------- | ------------- | ------------ |
| Dense (senza Global Pooling) | \~900.000        | 84.9%         | Maggiore     |
| Global Max Pooling           | \~94.000         | 87.9%         | Minore       |

âœ… **Global Max Pooling**:

* ğŸï¸ Addestramento piÃ¹ veloce
* ğŸ§  Maggiore capacitÃ  di **propagare l'errore nei primi layer**
* ğŸ§¼ Minor rischio di overfitting grazie alla riduzione dei parametri
* ğŸ§® PiÃ¹ efficiente in memoria e computazione

---

## ğŸ’¡ Considerazioni finali

* ğŸ“‰ Rimuovere i layer `Dense` riduce drasticamente i parametri
* âš¡ La rete risulta piÃ¹ veloce e **generalizza meglio** in fase di test
* ğŸ” `Global Max Pooling`:

  * ğŸ§® Nessun parametro da addestrare
  * ğŸ”¬ Seleziona la feature predominante per ogni mappa
* ğŸš€ La **test accuracy** migliora di 3 punti percentuali!

---
